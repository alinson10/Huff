Questions


1.) How does huffman coding work. Why is it lossless?

Usually every letter is encoded using binary representation of ascii value for a letter (8bits per letter).  Huffman coding uses the frequency of a letter in
the file to compress and a binary search tree to create the binary representation of a letter.  The letters are the leaves of the tree. A left edge is a 0 and 
a right edge is a 1. The edge path to a certain letter, creates the binary representation. (i.e left edge->right edge->left edge = 010). The more frequent the 
letter is in the file, the distance from the root is less. Therefore in compression, more frequent letters take less binary digits to encode. Overall the number 
of binary digits is less then the regular encoding.

This is lossless because no information about letters is lost, but rather represented differently. Therefore ascii "a" could be represented as 1100001 , but using
huffman bst "a" can be represented as 010. They represent the same exact information.

2.) How did you calc the compression-ratio? What length did you calculate block code?

The compression ratio was calculated using (number of bits of a file using Huffman encoding) / (number of bits of a file using Blockcode encoding).

To get the Huffman encoding size: Using the Huffman BST, find how many bits each word is. For the file, add together the bit size of everyword.

To get the Block Code encoding size: Find the number of words in the keyword list (all unique words from all the files). To get a code word length use 
log(number of words)/logbase2 to get the number of bits needed to encode the number of words. For the file, multiple the number  of bits per word by the number
of words in the file.

3.) What happens to the minimum, maximum, and average compression ratios when you increase the number of documents included in the set from which you
 build the code? Why?
 
 The minimum gets smaller, the maximum gets smaller, and average gets smaller. This happens because there is more content to discover
 which words are more frequent. Therefore if the more common words get increased frequency, then it will be closer to the root
 in the Huffman BST: Decreasing the bit size of a certain word and overall decreasing the size of the encoding and compression
 ratio.

4.) Pick one of the codes that you produced. For that code, what is the length of the shortest code word? the longest? 
Does the number of documents used to construct the code affect these numbers?
 
Choose old300. The code words are all the same length.  The number of documents to construct the dictionary for the code increases or decreases the
number of words, therefore decreases or increases the word length because more words means more bits are needed to represent a word. The number of documents
for the subset and frequency of the dictionary does not change the word length. It increases the frequency of certain words, but does not increase or decrease
the number of words in the dictionary.

5.)For the code constructed from all of the speeches, find the ten speeches with the worst compression ratio and the ten speeches with the best compression ratio. 
Can you explain why their compression ratio is good or bad?

The good compressions: Most of these are the oldest speeches. They have better compressions because the speech contains more of the simple commonly used words.

The bad compressions: Mostly modern speeches. They have bad compression because they use uncommon words.


6.) What does it mean for Huffman codes to be prefix codes? How would you decode a document encoded using a Huffman code?

Huffman codes are prefix codes because they are unique codes representing certain words.
The method of decoding Huffman:  The bit coding for a certain word is calculated by finding the path of a word in the huffmanBST 
and storing it in a hashtable where the key is the word. When it comes to decoding Huffman, simply search through the hashtable
for the specific bit sequence, and the key is the word that is decoded.

7.) If you worked as a team, what did each of the team members contribute?
Andrew Linson: Questions, Run the tests/Spreadsheet, created Huffman.java (Main class), created huffComparator.java
Gooi Liang Zheng:  Questions, Run the tests/Spreadsheet, created HuffNode.java, created BSTHuff.java, created Word.java


